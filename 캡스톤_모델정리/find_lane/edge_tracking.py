#!/usr/bin/env python3
"""
ê°œì„ ëœ Edge ê¸°ë°˜ ì°¨ì„  ë³€ê²½ ê°ì§€
================================
í•˜ë‹¨ ì¤‘ì•™ì˜ edge ìœ„ì¹˜ ì¶”ì  ë° ì´ë™ ë°©í–¥ ê°ì§€
"""
import cv2
import numpy as np
from pathlib import Path
import json

def load_roi_config(video_name: str, output_dir: Path):
    config_path = output_dir / f"{video_name}_roi_config.json"
    if not config_path.exists():
        return None
    with open(config_path, 'r') as f:
        config = json.load(f)
    return np.float32(config['roi_points'])

def detect_edges(bev_frame, exclude_bottom_height=100):
    """Edge ê²€ì¶œ"""
    h, w = bev_frame.shape[:2]
    
    gray = cv2.cvtColor(bev_frame, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)
    edges = cv2.Canny(blurred, 50, 150)
    
    # í•˜ë‹¨ ë§ˆìŠ¤í‚¹
    mask = np.ones_like(edges)
    mask[h - exclude_bottom_height:, :] = 0
    edges = cv2.bitwise_and(edges, mask)
    
    return edges

def find_bottom_edge_position(edges, bottom_roi_height=120, exclude_bottom=100):
    """
    í•˜ë‹¨ ì¤‘ì•™ì˜ edge ìœ„ì¹˜ ì°¾ê¸°
    
    Returns:
        edge_x: Edgeì˜ x ì¢Œí‘œ (ì—†ìœ¼ë©´ None)
        edge_strength: Edge ê°•ë„
    """
    h, w = edges.shape
    
    # í•˜ë‹¨ ROI ì„¤ì • (í•¸ë“¤ ë°”ë¡œ ìœ„)
    roi_y_start = h - exclude_bottom - bottom_roi_height
    roi_y_end = h - exclude_bottom
    
    # ì¤‘ì•™ ë¶€ê·¼ë§Œ ì²´í¬ (ì „ì²´ í­ì˜ 20-80%)
    roi_x_start = int(w * 0.2)
    roi_x_end = int(w * 0.8)
    
    roi = edges[roi_y_start:roi_y_end, roi_x_start:roi_x_end]
    
    if roi.sum() == 0:
        return None, 0.0
    
    # ê° ì—´ì˜ edge í”½ì…€ ìˆ˜
    column_sums = roi.sum(axis=0)
    
    # Edgeê°€ ê°€ì¥ ê°•í•œ ìœ„ì¹˜ ì°¾ê¸°
    if column_sums.max() == 0:
        return None, 0.0
    
    # ìƒìœ„ 20% ì´ìƒì˜ edgeë§Œ ê³ ë ¤
    threshold = column_sums.max() * 0.2
    significant_columns = np.where(column_sums > threshold)[0]
    
    if len(significant_columns) == 0:
        return None, 0.0
    
    # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ edge ì¤‘ì‹¬ ê³„ì‚°
    weights = column_sums[significant_columns]
    edge_x_relative = np.average(significant_columns, weights=weights)
    
    # ì‹¤ì œ x ì¢Œí‘œë¡œ ë³€í™˜
    edge_x = roi_x_start + int(edge_x_relative)
    
    # Edge ê°•ë„ (0-1)
    edge_strength = column_sums.max() / (roi.shape[0] * 255)
    
    return edge_x, edge_strength

def detect_lane_change_from_edge_movement(edge_position_history, min_movement=40):
    """
    Edge ìœ„ì¹˜ ì´ë ¥ìœ¼ë¡œ ì°¨ì„  ë³€ê²½ ê°ì§€
    
    Args:
        edge_position_history: [(frame, x_position), ...]
        min_movement: ìµœì†Œ ì´ë™ ê±°ë¦¬ (í”½ì…€)
    
    Returns:
        (is_changing, direction)
        direction: 'left_to_right', 'right_to_left', None
    """
    if len(edge_position_history) < 10:
        return False, None
    
    # ìµœê·¼ 30í”„ë ˆì„ (ì•½ 1ì´ˆ)
    recent = edge_position_history[-30:]
    
    # í•„í„°ë§: None ì œê±°
    valid_positions = [(f, x) for f, x in recent if x is not None]
    
    if len(valid_positions) < 5:
        return False, None
    
    # ì‹œì‘ê³¼ ë ìœ„ì¹˜
    start_positions = [x for _, x in valid_positions[:5]]
    end_positions = [x for _, x in valid_positions[-5:]]
    
    start_avg = np.mean(start_positions)
    end_avg = np.mean(end_positions)
    
    movement = end_avg - start_avg
    
    # ì´ë™ì´ ì¶©ë¶„íˆ í°ì§€ í™•ì¸
    if abs(movement) > min_movement:
        if movement > 0:
            return True, 'left_to_right'  # ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ
        else:
            return True, 'right_to_left'  # ì˜¤ë¥¸ìª½ì—ì„œ ì™¼ìª½ìœ¼ë¡œ
    
    return False, None

def process_video_with_edge_tracking(
    video_path: Path,
    output_path: Path,
    src_points: np.ndarray,
    lane_change_start_sec: float,
    lane_change_end_sec: float
):
    """Edge ìœ„ì¹˜ ì¶”ì  ê¸°ë°˜ ì°¨ì„  ë³€ê²½ ê°ì§€"""
    print(f"\nğŸ¥ Processing: {video_path.name}")
    print(f"   Expected lane change: {lane_change_start_sec}s ~ {lane_change_end_sec}s")
    
    # BEV ë³€í™˜
    dst_points = np.float32([[0, 0], [400, 0], [400, 600], [0, 600]])
    M = cv2.getPerspectiveTransform(src_points, dst_points)
    
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open video: {video_path}")
    
    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    writer = cv2.VideoWriter(str(output_path), fourcc, fps, (800, 600))
    
    frame_count = 0
    edge_position_history = []
    lane_change_events = []
    
    lc_start_frame = int(lane_change_start_sec * fps)
    lc_end_frame = int(lane_change_end_sec * fps)
    
    print(f"\nì²˜ë¦¬ ì‹œì‘...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        current_time = frame_count / fps
        
        # BEV ë³€í™˜
        bev_frame = cv2.warpPerspective(frame, M, (400, 600))
        
        # Edge ê²€ì¶œ
        edges = detect_edges(bev_frame, 100)
        
        # í•˜ë‹¨ edge ìœ„ì¹˜ ì°¾ê¸°
        edge_x, edge_strength = find_bottom_edge_position(edges, bottom_roi_height=120, exclude_bottom=100)
        
        # ì´ë ¥ ì €ì¥
        edge_position_history.append((frame_count, edge_x))
        if len(edge_position_history) > 150:
            edge_position_history = edge_position_history[-150:]
        
        # ì°¨ì„  ë³€ê²½ ê°ì§€
        is_changing, direction = detect_lane_change_from_edge_movement(edge_position_history, min_movement=40)
        
        if is_changing:
            lane_change_events.append((frame_count, direction))
        
        # ì‹œê°í™”
        # ì¢Œì¸¡: BEV + Edge
        left_panel = bev_frame.copy()
        edge_color = np.zeros_like(left_panel)
        edge_color[edges > 0] = [0, 0, 255]
        left_panel = cv2.addWeighted(left_panel, 0.7, edge_color, 0.3, 0)
        
        # í•˜ë‹¨ ROI í‘œì‹œ
        h, w = left_panel.shape[:2]
        roi_y_start = h - 100 - 120
        roi_y_end = h - 100
        roi_x_start = int(w * 0.2)
        roi_x_end = int(w * 0.8)
        
        cv2.rectangle(left_panel, (roi_x_start, roi_y_start), (roi_x_end, roi_y_end), (0, 255, 0), 2)
        
        # Edge ìœ„ì¹˜ í‘œì‹œ
        if edge_x is not None:
            cv2.circle(left_panel, (edge_x, (roi_y_start + roi_y_end) // 2), 8, (255, 0, 255), -1)
            cv2.line(left_panel, (edge_x, roi_y_start), (edge_x, roi_y_end), (255, 0, 255), 2)
        
        # ìš°ì¸¡: Edgeë§Œ
        right_panel = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        cv2.rectangle(right_panel, (roi_x_start, roi_y_start), (roi_x_end, roi_y_end), (0, 255, 0), 2)
        
        if edge_x is not None:
            cv2.circle(right_panel, (edge_x, (roi_y_start + roi_y_end) // 2), 8, (255, 0, 255), -1)
        
        # í•©ì„±
        combined = np.hstack([left_panel, right_panel])
        
        # ì •ë³´ ë°•ìŠ¤
        in_lane_change_zone = lc_start_frame <= frame_count <= lc_end_frame
        
        cv2.rectangle(combined, (5, 5), (795, 150), (0, 0, 0), -1)
        cv2.rectangle(combined, (5, 5), (795, 150), (255, 255, 255), 2)
        
        y_offset = 30
        cv2.putText(combined, f"Time: {current_time:.2f}s", 
                   (15, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        y_offset += 30
        
        # Edge ìœ„ì¹˜
        if edge_x is not None:
            cv2.putText(combined, f"Edge X: {edge_x}px  Strength: {edge_strength:.3f}", 
                       (15, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        else:
            cv2.putText(combined, "Edge: Not detected", 
                       (15, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)
        y_offset += 30
        
        # ì°¨ì„  ë³€ê²½ ìƒíƒœ
        if is_changing:
            direction_text = "LEFTâ†’RIGHT" if direction == 'left_to_right' else "RIGHTâ†’LEFT"
            cv2.putText(combined, f">>> {direction_text} <<<", 
                       (15, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
        
        # ì˜ˆìƒ êµ¬ê°„
        if in_lane_change_zone:
            cv2.rectangle(combined, (10, 550), (790, 590), (0, 165, 255), -1)
            cv2.putText(combined, "Expected Lane Change Zone", 
                       (200, 575), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # ìµœê·¼ ê°ì§€
        if frame_count in [f for f, _ in lane_change_events[-10:]]:
            cv2.circle(combined, (750, 50), 20, (0, 0, 255), -1)
            cv2.putText(combined, "DETECTED", 
                       (620, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        writer.write(combined)
        
        frame_count += 1
        if frame_count % 30 == 0:
            status = "CHANGE_ZONE" if in_lane_change_zone else "NORMAL"
            edge_str = f"x={edge_x}" if edge_x else "none"
            change_str = direction if is_changing else "normal"
            print(f"  Frame {frame_count}/{total_frames} ({current_time:.1f}s) [{status}] edge:{edge_str} {change_str}")
    
    cap.release()
    writer.release()
    
    # ë¶„ì„
    print(f"\nâœ… Output saved: {output_path}")
    print(f"\nğŸ“Š ì°¨ì„  ë³€ê²½ ê°ì§€ ë¶„ì„:")
    
    if lane_change_events:
        # ì—°ì†ëœ ì´ë²¤íŠ¸ ê·¸ë£¹í•‘
        groups = []
        current_group = {'start': lane_change_events[0][0], 'end': lane_change_events[0][0], 'direction': lane_change_events[0][1]}
        
        for i in range(1, len(lane_change_events)):
            frame, direction = lane_change_events[i]
            if frame - lane_change_events[i-1][0] <= 5 and direction == current_group['direction']:
                current_group['end'] = frame
            else:
                groups.append(current_group)
                current_group = {'start': frame, 'end': frame, 'direction': direction}
        
        groups.append(current_group)
        
        print(f"   ê°ì§€ëœ ì°¨ì„  ë³€ê²½: {len(groups)}íšŒ")
        for i, group in enumerate(groups):
            start_time = group['start'] / fps
            end_time = group['end'] / fps
            dir_text = "ì™¼ìª½â†’ì˜¤ë¥¸ìª½" if group['direction'] == 'left_to_right' else "ì˜¤ë¥¸ìª½â†’ì™¼ìª½"
            in_expected = lc_start_frame <= group['start'] <= lc_end_frame
            status = "âœ… ì˜ˆìƒ êµ¬ê°„" if in_expected else "âš ï¸ ì˜ˆìƒ ì™¸"
            duration = group['end'] - group['start'] + 1
            print(f"     {i+1}. {start_time:.2f}s~{end_time:.2f}s ({duration}f) {dir_text} {status}")
    else:
        print("   ê°ì§€ ì—†ìŒ")
    
    return lane_change_events

def main():
    print("="*70)
    print(" ê°œì„ ëœ Edge ê¸°ë°˜ ì°¨ì„  ë³€ê²½ ê°ì§€ (ìœ„ì¹˜ ì¶”ì )")
    print("="*70)
    
    data_dir = Path(__file__).resolve().parent.parent / "Data"
    output_dir = Path(__file__).resolve().parent
    
    videos = [
        {
            "path": data_dir / "ì´ë²¤íŠ¸ 4.mp4",
            "name": "ì´ë²¤íŠ¸ 4",
            "lane_change_start": 4.0,
            "lane_change_end": 6.0,
        },
        {
            "path": data_dir / "ì´ë²¤íŠ¸ 5.mp4",
            "name": "ì´ë²¤íŠ¸ 5",
            "lane_change_start": 5.0,
            "lane_change_end": 8.0,
        }
    ]
    
    for video_info in videos:
        print(f"\n{'='*70}")
        print(f"ğŸ¬ {video_info['name']}")
        print(f"{'='*70}")
        
        roi_points = load_roi_config(video_info['name'], output_dir)
        if roi_points is None:
            print(f"âŒ ROI config not found")
            continue
        
        output_path = output_dir / f"{video_info['name']}_edge_tracking.mp4"
        
        try:
            process_video_with_edge_tracking(
                video_info['path'],
                output_path,
                roi_points,
                video_info['lane_change_start'],
                video_info['lane_change_end']
            )
        except Exception as e:
            print(f"âŒ Error: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "="*70)
    print("âœ… Edge ì¶”ì  ê¸°ë°˜ ì°¨ì„  ë³€ê²½ ê°ì§€ ì™„ë£Œ!")
    print("="*70)

if __name__ == "__main__":
    main()
